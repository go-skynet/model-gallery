- config_file:
    backend: llama
    context_size: 1024
    parameters:
      model: goliath-120b.Q2_K.gguf
    template:
      chat: Vicuna-Short
      completion: Vicuna-Short
  description: TheBloke/goliath-120b-GGUF - llama configuration
  files:
  - filename: Vicuna-Short.tmpl
    sha256: 4f2e4df7367ee6f177bcb043807911366ca9b4d17bed618d08f34ebb89c50117
    uri: 
      https://raw.githubusercontent.com/dave-gray101/model-gallery/main/prompt-templates/Vicuna-Short.tmpl
  - filename: goliath-120b.Q2_K.gguf
    sha256: 24b0fd1bc571b349cf0e0a76ac91cce785d939af6177d698e630aff788d2370a
    uri: 
      https://huggingface.co/TheBloke/goliath-120b-GGUF/resolve/main/goliath-120b.Q2_K.gguf
  icon: https://huggingface.co/front/assets/huggingface_logo-noborder.svg
  license: llama2
  name: TheBloke__goliath-120b-GGUF__goliath-120b.Q2_K.gguf
  tags:
  - transformers
  - llama
  - conversational
  - en
  - base_model:alpindale/goliath-120b
  - license:llama2
  - text-generation-inference
  - region:us
  urls:
  - https://huggingface.co/TheBloke/goliath-120b-GGUF
